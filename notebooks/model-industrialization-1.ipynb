{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3749108f",
   "metadata": {},
   "source": [
    "<center><h1 style=\"color:#CC0099\">House-prices-modeling</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a7359f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:07.594647Z",
     "start_time": "2023-05-05T21:38:05.310267Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad45e4d",
   "metadata": {},
   "source": [
    "<h1>Model Building: <span style=\"color:#6666CC\">Model Training</span></h1>\n",
    "\n",
    "### 1. Dataset loading and splitting into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7ee847",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:10.984268Z",
     "start_time": "2023-05-05T21:38:10.951097Z"
    }
   },
   "outputs": [],
   "source": [
    "label_col = 'SalePrice'\n",
    "useful_features = ['Foundation', 'KitchenQual', 'TotRmsAbvGrd', 'WoodDeckSF', 'YrSold', '1stFlrSF']\n",
    "\n",
    "continuous_columns = ['TotRmsAbvGrd', 'WoodDeckSF', 'YrSold', '1stFlrSF']\n",
    "categorical_columns = ['Foundation', 'KitchenQual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a52cfe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:11.842322Z",
     "start_time": "2023-05-05T21:38:11.797724Z"
    }
   },
   "outputs": [],
   "source": [
    "def reading_csv(file_path: str) -> pd.DataFrame:\n",
    "    csv_file = pd.read_csv(file_path)\n",
    "    return csv_file\n",
    "\n",
    "train_csv = reading_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3b16a63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:12.528519Z",
     "start_time": "2023-05-05T21:38:12.517825Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitting_df(initial_df: pd.DataFrame):\n",
    "    train_df, test_df = train_test_split(initial_df, test_size=0.33, random_state=42)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = splitting_df(train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec5919f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:12.992948Z",
     "start_time": "2023-05-05T21:38:12.987448Z"
    }
   },
   "outputs": [],
   "source": [
    "def useful_df(data_df: pd.DataFrame, useful_features: List[str], label_col: str) -> pd.DataFrame:\n",
    "    # Final df with only the useful features and the label\n",
    "    data_df = data_df[useful_features + [label_col]]\n",
    "    return data_df\n",
    "\n",
    "train_df = useful_df(train_df, useful_features, label_col)\n",
    "test_df = useful_df(test_df, useful_features, label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f0b0b",
   "metadata": {},
   "source": [
    "### 2. Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00aebc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:14.246502Z",
     "start_time": "2023-05-05T21:38:14.224495Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(data: pd.DataFrame, categorical_columns: List[str], continuous_columns: List[str]) -> pd.DataFrame:\n",
    "    # Replace missing values in categorical columns with mode\n",
    "    data[categorical_columns] = data[categorical_columns].fillna(data[categorical_columns].mode().iloc[0])\n",
    "    # Replace missing values in continuous columns with mean\n",
    "    data[continuous_columns] = data[continuous_columns].fillna(data[continuous_columns].mean())\n",
    "    # Remove duplicated rows\n",
    "    data = data.drop_duplicates(keep='first')\n",
    "    # Reset index\n",
    "    preprocess_df = data.reset_index(drop=True)\n",
    "\n",
    "    return preprocess_df\n",
    "\n",
    "train_df = preprocessing(train_df, categorical_columns, continuous_columns)\n",
    "test_df = preprocessing(test_df, categorical_columns, continuous_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32a4d2",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#084A68\">Saving the scaler and encoder</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f45fb48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:14.637631Z",
     "start_time": "2023-05-05T21:38:14.622273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/encoder.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_scaler_encoder(df_train: pd.DataFrame, categorical_columns: List[str], \n",
    "                       continuous_columns: List[str]) -> Tuple[StandardScaler, OneHotEncoder]:\n",
    "    # Fitting the scaler on the train_df\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train[continuous_columns])\n",
    "\n",
    "    # Fitting the encoder on the train_df\n",
    "    encoder = OneHotEncoder()\n",
    "    encoder.fit(df_train[categorical_columns])\n",
    "\n",
    "    return scaler, encoder\n",
    "            \n",
    "scaler, encoder = fit_scaler_encoder(train_df, categorical_columns, continuous_columns)\n",
    "\n",
    "# Saving the scaler and the encoder to use them after for transforming\n",
    "models_folder = '../models'\n",
    "joblib.dump(scaler, os.path.join(models_folder, 'scaler.joblib'))\n",
    "joblib.dump(encoder, os.path.join(models_folder, 'encoder.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6584536e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:15.023744Z",
     "start_time": "2023-05-05T21:38:14.986966Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_scaler_encoder(data_df: pd.DataFrame, categorical_columns: List[str], \n",
    "                             continuous_columns: List[str], models_folder: str) -> pd.DataFrame:\n",
    "    # Using the saved encoder and scalers\n",
    "    scaler = joblib.load(os.path.join(models_folder, 'scaler.joblib'))\n",
    "    encoder = joblib.load(os.path.join(models_folder, 'encoder.joblib'))\n",
    "    scaled_data = scaler.transform(data_df[continuous_columns])\n",
    "    scaled_df = pd.DataFrame(data=scaled_data, columns=continuous_columns)\n",
    "    encoded_data = encoder.transform(data_df[categorical_columns])\n",
    "    encoded_df = pd.DataFrame(data=encoded_data.toarray(),\n",
    "                              columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "    transformed_df = pd.concat([scaled_df, encoded_df], axis=1)\n",
    "\n",
    "    return transformed_df\n",
    " \n",
    "X_train = transform_scaler_encoder(train_df, categorical_columns, continuous_columns, models_folder)\n",
    "y_train = train_df[label_col]\n",
    "\n",
    "X_test = transform_scaler_encoder(test_df, categorical_columns, continuous_columns, models_folder)\n",
    "y_test = test_df[label_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd99c8",
   "metadata": {},
   "source": [
    "<h1>Model Building: <span style=\"color:#6666CC\">Model Training and Evaluation</span></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ceffc4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T21:38:17.932470Z",
     "start_time": "2023-05-05T21:38:17.897925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/model.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_rmsle(y_test: np.ndarray, y_pred: np.ndarray, precision: int = 2) -> float:\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_test, y_pred))\n",
    "    return round(rmsle, precision)\n",
    "\n",
    "def build_model(X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> dict:\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmsle = compute_rmsle(y_test, y_pred)\n",
    "    \n",
    "    return {'model': model, 'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'rmsle': rmsle}\n",
    "\n",
    "result = build_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# save the model to a file called model.joblib in the models folder\n",
    "model = result['model']\n",
    "joblib.dump(model, os.path.join(models_folder, 'model.joblib'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e61f0c",
   "metadata": {},
   "source": [
    "<h1 style=\"color:#6666CC\">Model inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e63cb61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120547.89852744, 206552.7998175 , 165279.21985763, ...,\n",
       "       181477.99967176, 158810.53764499, 204258.36547822])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_test = reading_csv('../data/test.csv')\n",
    "house_test = preprocessing(house_test, categorical_columns, continuous_columns)\n",
    "test_dataset = transform_scaler_encoder(house_test, categorical_columns, continuous_columns, models_folder)\n",
    "\n",
    "def make_predictions(test_dataset: pd.DataFrame, models_folder: str) -> np.ndarray:\n",
    "    # using joblib.load to make the predictions\n",
    "    model = joblib.load(os.path.join(models_folder, 'model.joblib'))\n",
    "    predictions = model.predict(test_dataset)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "make_predictions(test_dataset, models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0f02ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
